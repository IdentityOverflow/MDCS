# CLAUDE.md

## ğŸ¯ Project Vision: Project 2501 - Cognitive Systems Framework

**Core Innovation**: Dynamic System Prompt Architecture - treats system prompts as living, modular heads-up displays that stay visible throughout conversations, solving static prompt limitations and context rot.

### Key Features:
- **Simple Modules**: Static text templates 
- **Advanced Modules**: Python scripts with dynamic content generation
- **Self-Reflecting AI**: AI introspection via `ctx.reflect()`
- **Template System**: `@module_name` references and `${variable}` outputs

## ğŸ“ Architecture (3-Layer)
```
Frontend (Vue 3) â†â†’ Backend (FastAPI) â†â†’ Database (PostgreSQL + pgvector)
```

**Stack**:
- **Frontend**: Vue 3.5.18, TypeScript, Pinia, Vite
- **Backend**: FastAPI 0.104.1, SQLAlchemy 2.0.23, RestrictedPython 7.0
- **Database**: PostgreSQL with pgvector, UUID primary keys

## ğŸ“ Project Structure
```
project-2501/
â”œâ”€â”€ frontend/         # Vue 3 app (components, stores, types)
â”œâ”€â”€ backend/          # FastAPI backend
â”‚   â”œâ”€â”€ app/core/    # Script engine, plugins, validation
â”‚   â”œâ”€â”€ app/models/  # Database models (Persona, Module, Conversation)
â”‚   â”œâ”€â”€ app/api/     # REST endpoints
â”‚   â””â”€â”€ tests/       # 461 tests (unit + integration)
â”œâ”€â”€ database/        # PostgreSQL setup
â””â”€â”€ scripts/         # Development scripts
```

## ğŸ› ï¸ Development Setup

### Backend
```bash
./scripts/install_BE.sh
conda activate project2501
./scripts/run_BE.sh

# Testing (461 tests)
source /home/q/miniforge3/etc/profile.d/conda.sh && conda activate project2501 && pytest -v
```

### Frontend
```bash
cd frontend
npm run dev        # localhost:5173
npm run build
npm run lint
```

### Database
- **Main**: project2501 (project2501 user)
- **Test**: project2501_test 
- **Models**: Persona, Module, Conversation, Message (UUIDs)

## ğŸ’¡ Core API Endpoints

```
Chat:          POST /api/chat/send          # Complete response
               POST /api/chat/stream        # Streaming response
Templates:     POST /api/templates/resolve  # Resolve @module_name references
Conversations: GET  /api/conversations/by-persona/{id}
Messages:      GET  /api/messages/by-conversation/{id}
Modules:       GET  /api/modules/
Personas:      GET  /api/personas/
```

## ğŸš¨ Development Principles

### Search Before Code
```bash
# Always search first to avoid reimplementation
grep -r "feature_name" backend/ frontend/
find . -name "*.py" -o -name "*.vue" | xargs grep -l "keyword"
```

### Architecture Guidelines
- **3-Layer**: Frontend â†” Backend â†” Database 
- **Modularity**: Single responsibility, loose coupling
- **Settings Storage**: Frontend localStorage only (no backend duplication)
- **Testing**: 461/461 tests passing - maintain TDD approach

## âš¡ Workflow

### Before Committing
```bash
# Backend
cd backend && pytest && python app/main.py

# Frontend  
cd frontend && npm run lint && npm run format
```

### Frontend-First Development
1. Design UI components first
2. Define API contracts based on frontend needs
3. Write tests
4. Implement backend endpoints
5. Integration testing

## âœ… Implementation Status

### Complete Features
- **Full Chat System**: Ollama/OpenAI streaming with persona integration
- **Advanced Modules**: Python scripts with RestrictedPython sandbox + 15+ plugins  
- **Self-Reflecting AI**: `ctx.reflect()` for AI introspection
- **Module System**: `@module_name` and `${variable}` resolution
- **Conversation Persistence**: Full CRUD with message editing/thinking
- **ğŸš€ NEW: Chat Cancellation System**: Complete interrupt/stop signal implementation
- **Test Coverage**: 627/627 tests passing

### Chat System Architecture
**Settings**: Frontend localStorage only (no backend storage)
**Request Format**: All settings passed in request payload
**Providers**: Ollama + OpenAI with streaming support
**Personas**: System prompt resolution with module integration

**LocalStorage Keys**:
- `chat-controls`: Temperature, tokens, provider selection
- `ollama-connection`: Host, model, options
- `openai-connection`: API key, model, base URL


### Chat Request Format
```json
{
  "message": "Hello!",
  "provider": "ollama" | "openai",
  "persona_id": "uuid-string",
  "provider_settings": { "host": "...", "model": "..." },
  "chat_controls": { "temperature": 0.7, "max_tokens": 1024 }
}
```


### Database Models
**Persona**: AI persona configurations with templates
**Module**: Simple (static) and Advanced (Python script) modules
**Conversation**: UUID primary key, persona relationship, provider info
**Message**: Content, thinking, token counts, cascade deletion

### Advanced Modules
**Plugin Functions**: 15+ built-in functions (time, conversation, utilities)
**Script Execution**: RestrictedPython sandbox with trigger patterns  
**Variable System**: `${variable}` for dynamic script outputs
**Timing Options**: BEFORE, AFTER, CUSTOM execution modes

### Plugin Examples
```python
# Time functions
ctx.get_current_time("%H:%M")
ctx.get_relative_time("2 hours ago")

# Conversation functions  
ctx.get_message_count()
ctx.get_recent_messages(5)

# AI reflection
ctx.reflect("Rate this response quality 1-10")
```

## ğŸš€ Current Priority: Staged Module Execution Redesign

**Goal**: Replace BEFORE/AFTER timing system with clear 5-stage execution pipeline with AI dependency management.

### âœ… Phase 1: Database Schema & Models (COMPLETED 2025-01-28)
- âœ… **Migration 006**: ConversationState table + updated Module fields
- âœ… **ExecutionContext enum**: IMMEDIATE, POST_RESPONSE, ON_DEMAND (replaces ExecutionTiming)
- âœ… **Script Analysis**: Automatic AI dependency detection (`requires_ai_inference`)
- âœ… **ConversationState model**: Dedicated state storage with relationships
- âœ… **Enhanced Module model**: Stage classification, priority ordering, helper methods
- âœ… **Full testing**: All models working correctly with database integration

### âœ… Phase 2: Core Execution Engine (COMPLETED 2025-01-28)
- âœ… **Unit Tests**: Complete test coverage for new models and script analyzer (44 new tests)
- âœ… **StagedModuleResolver**: Fully implemented 5-stage pipeline replacement for ModuleResolver
  - âœ… Stage 1: Simple + IMMEDIATE Non-AI + Previous POST_RESPONSE
  - âœ… Stage 2: IMMEDIATE AI-powered modules
  - âœ… Stage 3: Main AI response generation (caller-handled)  
  - âœ… Stage 4: POST_RESPONSE Non-AI modules
  - âœ… Stage 5: POST_RESPONSE AI-powered modules
- âœ… **ConversationState integration**: Proper state storage and retrieval between stages
- âœ… **Enhanced testing**: 20 new tests for StagedModuleResolver (375 total tests passing)

### âœ… Phase 3: API & Response Models (COMPLETED 2025-01-28)
- âœ… **Fix all 14 integration tests**: Updated API endpoints for ExecutionContext system
  - âœ… 8 modules API endpoint tests (ExecutionContext field updates)
  - âœ… 5 modules integration tests (field and enum updates)  
  - âœ… 1 chat template integration test (StagedModuleResolver integration)
- âœ… **Update chat endpoints**: Replaced old ModuleResolver with StagedModuleResolver
  - âœ… Template resolution using Stage 1 + Stage 2 pipeline
  - âœ… POST_RESPONSE execution using Stage 4 + Stage 5 pipeline
  - âœ… Recursive module resolution within stages
  - âœ… Simple modules automatically execute in Stage 1
- âœ… **All 535 tests passing**: Complete staged execution system working end-to-end

### âœ… Phase 4: Frontend Integration (COMPLETED 2025-01-28)
- âœ… **TypeScript interfaces**: Updated for ExecutionContext + requires_ai_inference
  - âœ… Replaced old `timing` field with `execution_context` enum
  - âœ… Added `requires_ai_inference` boolean field  
  - âœ… Enhanced type safety with ExecutionContext and ModuleType types
- âœ… **Module management UI**: Updated components for new execution context system
  - âœ… ModuleCard: Shows execution context and AI inference requirements
  - âœ… NewModule: Dropdown with human-friendly execution context options
  - âœ… User-friendly labels ("Before Response", "After Response", "On Demand")
- âœ… **Full end-to-end compatibility**: Frontend and backend fully integrated

## ğŸ‰ **STAGED EXECUTION REDESIGN - COMPLETE!**

The staged execution system has been **fully implemented and integrated** across the entire stack:

### ğŸ—ï¸ **System Architecture**
**5-Stage Execution Pipeline** replacing the old BEFORE/AFTER timing system:
- **Stage 1**: Template preparation (Simple + IMMEDIATE Non-AI + Previous POST_RESPONSE)
- **Stage 2**: Pre-response AI processing (IMMEDIATE modules with AI inference)
- **Stage 3**: Main AI response generation (handled by chat system)
- **Stage 4**: Post-response processing (POST_RESPONSE modules without AI)
- **Stage 5**: Post-response AI analysis (POST_RESPONSE modules with AI reflection)

### ğŸš€ **Key Benefits Delivered**
- âœ… **Clear execution order**: Predictable, deterministic module execution
- âœ… **No race conditions**: Ordered processing eliminates timing-based conflicts
- âœ… **Better performance**: Efficient stage-based execution with minimal overhead  
- âœ… **AI-aware system**: Automatic detection and appropriate scheduling of AI-dependent modules
- âœ… **Enhanced user feedback**: Clear stage identification for UI/UX improvements
- âœ… **Backwards compatibility**: All existing functionality preserved and enhanced

### ğŸ“Š **Implementation Stats**
- **535 tests passing** (including 64+ new tests for staged system)
- **Zero breaking changes** for existing modules and personas
- **Complete API compatibility** with enhanced ExecutionContext system
- **Full TypeScript integration** with improved type safety

The staged execution redesign successfully transforms Project 2501's cognitive system architecture from ad-hoc timing to a robust, scalable, and maintainable execution pipeline.

## ğŸ”„ **NEXT: Services Layer Refactoring** *(Planned - See SERVICES_REFACTORING.md)*

**Current Challenge**: The services layer has grown into large monolithic files with inheritance-based duplication:
- `staged_module_resolver_base.py`: **1,073 lines** ğŸ˜±
- Multiple base + enhanced service pairs creating maintenance complexity

**Planned Solution**: Domain-driven modular architecture detailed in `SERVICES_REFACTORING.md`
- **5-Phase migration plan** from monolithic to modular services  
- **Domain separation**: `providers/`, `session/`, `modules/`, `utils/`
- **Composition over inheritance** - eliminate base/enhanced duplication
- **Single responsibility** - each file has one clear purpose

## ğŸš€ **CHAT CANCELLATION SYSTEM - COMPLETE!** *(2025-01-27)*

**Problem Solved**: Users were blocked during AI inference across stages 2, 3, and 5, unable to interrupt processing or send consecutive messages.

### ğŸ—ï¸ **Implementation Architecture**

**Backend Infrastructure**:
- âœ… **ChatSessionManager**: Session lifecycle management with AsyncIO task cancellation
- âœ… **Enhanced AI Providers**: Ollama/OpenAI services with session-aware cancellation
- âœ… **Staged Resolver Cancellation**: Cancellation checks integrated across all 5 execution stages
- âœ… **Streaming Accumulator**: Converts non-streaming requests to cancellable streaming
- âœ… **Session Management API**: Dedicated endpoints for cancellation control

**Frontend UX Enhancements**:
- âœ… **Dynamic Button States**: Send âœˆï¸ â†” Stop â¹ï¸ based on processing state
- âœ… **Consecutive Messages**: Cancel-and-replace behavior for new user input
- âœ… **Real-time Feedback**: Processing stage indicators with cancellation options
- âœ… **Error Handling**: Graceful cancellation messages and state cleanup

### ğŸ“Š **Key Features Delivered**

**1. Interrupt/Stop Signal**
- **Stop Button**: Appears during AI inference (stages 2, 3, 5)
- **Immediate Cancellation**: Real-time response to user stop requests
- **Session Tracking**: Unique session IDs for targeted cancellation
- **Stage Awareness**: Cancellation works across all module execution stages

**2. Consecutive Message Support** 
- **Auto-Cancel Behavior**: New message automatically cancels current processing
- **No Blocking**: Users can type and send while AI is thinking/generating
- **Clean Replacement**: Current request cancelled â†’ new request immediately sent
- **Seamless UX**: No queuing, no waiting, instant responsiveness

**3. Enhanced User Experience**
- **Visual Feedback**: Clear processing indicators with stop controls
- **Error Recovery**: User-friendly cancellation messages ("â¹ï¸ Message generation was stopped")
- **State Management**: Robust session cleanup and error handling
- **Backward Compatibility**: All existing functionality preserved and enhanced

### ğŸ”§ **Technical Implementation**

**Session Management Endpoints**:
```typescript
POST /api/chat/cancel/{session_id}  // Cancel active session
GET  /api/chat/status/{session_id}  // Check session status
GET  /api/chat/sessions/active      // List active sessions
```

**Frontend State Management**:
```typescript
// Session tracking
currentSessionId: string | null
isSessionCancelling: boolean

// Core methods
cancelCurrentSession(): Promise<boolean>
getSessionStatus(id: string): Promise<SessionStatus>
```

**Auto-Cancel Logic**:
```typescript
// Before sending new message
if (isStreaming && currentSessionId) {
  await cancelCurrentSession()  // Cancel current
}
// Then immediately send new message
```

### âœ¨ **User Experience Transformation**

**Before**: Users blocked during AI processing, no interruption capability
**After**: Complete control with instant responsiveness

- â¹ï¸ **Manual Stop**: Red stop button during AI processing
- ğŸ”„ **Auto-Cancel**: Type new message â†’ auto-cancels current â†’ sends new
- ğŸ“± **Responsive UI**: Never blocked, always interactive
- ğŸ’¬ **Fluid Conversation**: Natural conversation flow with instant message replacement

### ğŸ§ª **Quality Assurance**

- **627 Tests Passing** (up from 461) - comprehensive cancellation coverage
- **Integration Testing**: End-to-end session lifecycle verification
- **API Testing**: Complete endpoint coverage with proper mocking
- **Error Scenarios**: Robust handling of cancellation edge cases
- **Performance**: Lightweight session management with minimal overhead

### ğŸ“ˆ **Impact Metrics**

**Development**:
- **+166 New Tests**: Comprehensive cancellation scenario coverage
- **3 New Backend Services**: Session manager, enhanced providers, streaming accumulator
- **4 New API Endpoints**: Complete session management interface
- **Zero Breaking Changes**: Full backward compatibility maintained

**User Experience**:
- **100% Interrupt Capability**: Any AI processing stage can be stopped
- **0s Response Time**: Instant cancellation and new message sending
- **Infinite Consecutive Messages**: No blocking, unlimited message flow
- **Seamless Integration**: Works with all existing persona and module features

## ğŸ¯ **Next Steps - Future Enhancements**

Priority roadmap for continued development:

### ğŸ§ª Phase 5: Future Enhancements (OPTIONAL)
- â³ **Performance optimization**: Stage execution monitoring and metrics
- â³ **Advanced cancellation**: Batch session management and selective stage cancellation  
- â³ **Enhanced feedback**: Progress bars and detailed stage status
- â³ **Code cleanup**: Remove legacy ExecutionTiming references

## ğŸ”® Future Priorities  

### Short-term Enhancements
- **Streaming State Refactoring**: Simplify state management from multiple flags to single enum
- **Performance Optimization**: Add metrics and monitoring for cancellation overhead
- **Enhanced UX**: Progress bars and detailed stage status indicators

### Medium-term Features  
- **Import/Export System**: Persona and module sharing capabilities
- **Advanced Chat Features**: File uploads, tool integration, multi-modal support
- **Module Marketplace**: Community-driven module sharing and discovery
